"""
Reflection stage for the 2-agent ablation loop.

• Reads the latest proposal that `generation_pair` dropped in state.proposal[-1]
• Produces structured feedback
• If the feedback contains the *exact* sentinel phrase "Garde la peche"
  → the workflow terminates (goto=END)
• Otherwise the loop continues (goto="generation_pair")
"""

from typing import List, Literal
from langgraph.types import Command
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from data_models import PairState
from llm_models import pair_reflection_agent
from prompts import RE_PAIR_PROMPT
from IPython.display import display, Markdown
from langgraph.graph import MessagesState, StateGraph, START, END

def reflection_pair_node(state: PairState) -> Command[Literal["generation_pair", END]]:
    
    user_request = state.user_request
    proposal = state.proposal[-1]
    
    reflection_user_message = f"""
    This is the user request : {user_request} and this is the proposal generated by the generation agent: {proposal}
    Give it a construcive feedback so they can meet the target. Once you think the target is met, and only then, write
    'Garde la peche'.
    """
    
    base_model_output = pair_reflection_agent.invoke([
        SystemMessage(content=RE_PAIR_PROMPT),
        HumanMessage(content=reflection_user_message)
    ])

    display(Markdown(f"**Reflection Response:** {base_model_output.content}"))

    if 'Garde la peche' in base_model_output.content:
        return Command(
            update={
                "messages": [base_model_output.content],
                "feedback": [base_model_output.content],
            },
            goto=END
        )

    else:
        return Command(
            update={
                "messages": [base_model_output.content],
                "feedback": [base_model_output.content],
            },
            goto="generation_pair"
        )
