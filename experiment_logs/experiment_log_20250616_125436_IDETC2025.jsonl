{"config": {"llm_type": "reasoning", "temperature": 1.0, "workflow_type": "mas", "run_id": 0, "seed": 0}, "start_time": "2025-06-16T12:54:36.055326", "success": false, "error": "Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 128000 tokens. However, you requested 132424 tokens (68424 in the messages, 64000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}", "n_dsgs": 0, "wall_time": 1468.037528038025}
{"config": {"llm_type": "reasoning", "temperature": 1.0, "workflow_type": "mas", "run_id": 1, "seed": 1}, "start_time": "2025-06-16T13:19:04.093323", "success": true, "error": null, "n_dsgs": 2, "wall_time": 192.90942215919495}
{"config": {"llm_type": "reasoning", "temperature": 1.0, "workflow_type": "mas", "run_id": 2, "seed": 2}, "start_time": "2025-06-16T13:22:17.002822", "success": false, "error": "Could not parse response content as the length limit was reached - CompletionUsage(completion_tokens=64000, prompt_tokens=2755, total_tokens=66755, completion_tokens_details=None, prompt_tokens_details=None)", "n_dsgs": 0, "wall_time": 1315.805900812149}
{"config": {"llm_type": "reasoning", "temperature": 1.0, "workflow_type": "mas", "run_id": 3, "seed": 3}, "start_time": "2025-06-16T13:44:12.808957", "success": false, "error": "Recursion limit of 30 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT", "n_dsgs": 0, "wall_time": 186.97067523002625}
{"config": {"llm_type": "reasoning", "temperature": 1.0, "workflow_type": "mas", "run_id": 4, "seed": 4}, "start_time": "2025-06-16T13:47:19.779937", "success": true, "error": null, "n_dsgs": 2, "wall_time": 479.52396392822266}
